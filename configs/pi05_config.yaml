# PI0.5 Model Configuration
# Reference: https://github.com/Physical-Intelligence/openpi

model:
  # Backend: "openpi" (recommended) or "lerobot"
  backend: "openpi"
  
  # OpenPI settings (used when backend: "openpi")
  openpi_config: "pi05_libero"  # Config name for LIBERO tasks
  checkpoint_dir: null  # Auto-download if null, or specify path like "~/openpi/checkpoints/pi05_libero"
  
  # Alternative OpenPI configs:
  # - "pi05_droid" (DROID dataset)
  # - "pi05_aloha" (ALOHA hardware)
  # - "pi0_libero" (PI0, larger/slower)
  
  # LeRobot settings (used when backend: "lerobot")
  name: "lerobot/pi0_base"
  
  # Model architecture (for reference)
  type: "pi0.5"
  vision_encoder: "siglip"  # SigLIP for visual features  
  language_model: "gemma"  # Gemma-2B for VLA
  
  # Input specifications
  image_size: 256
  num_cameras: 1  # agentview only; can add robot0_eye_in_hand
  proprioception_dim: 8  # joint positions + gripper state
  
  # Action specifications  
  action_dim: 7  # 6 DOF + gripper
  action_horizon: 16  # Action chunking window
  
  # Memory optimization for 16GB GPU
  dtype: "bfloat16"  # bf16 for mixed precision
  load_in_8bit: false  # Enable if OOM
  gradient_checkpointing: true

inference:
  # Action chunking for efficiency
  chunk_size: 16
  temporal_ensemble: true
  ensemble_weights: "exponential"  # exponential decay for action blending
  
  # Uncertainty estimation
  compute_entropy: true
  entropy_threshold: 2.0  # Flag as "pre-fail" if entropy > threshold

device:
  cuda: true
  device_id: 0
